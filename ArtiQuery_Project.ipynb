{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "12GJ_wOgG6aylk3GLjbFQ-TTRGfTPYFRT",
      "authorship_tag": "ABX9TyO7AystnahOoB8TOvJOI1oJ"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install -q pyngrok"
      ],
      "metadata": {
        "id": "Y-2a0G1_bRbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q google-generativeai"
      ],
      "metadata": {
        "id": "h1n83InWhX8y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q unstructured"
      ],
      "metadata": {
        "id": "znscFuOo5CjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tiktoken"
      ],
      "metadata": {
        "id": "PzkJrUtb8u36"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q streamlit langchain openai faiss-cpu"
      ],
      "metadata": {
        "id": "jejSacjlHeeI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain_community"
      ],
      "metadata": {
        "id": "ukJ_iSxFHv4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OykTBNimxk8f",
        "outputId": "958b71ca-90ea-44e3-dfce-7aca499ddde7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from dotenv import load_dotenv\n",
        "\n",
        "# path the .env file on the drive\n",
        "path_env = '/content/drive/MyDrive/Colab Notebooks/ArtiQuery project/env'\n",
        "# Load environment variables\n",
        "load_dotenv(path_env)"
      ],
      "metadata": {
        "id": "UHESw1EWcRgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "403dc932-3139-4a5e-b73f-9b1fbf31b23f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q  langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-YJ4nRsjnal",
        "outputId": "96951901-4814-401e-f1a7-462882b4eb59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import os\n",
        "import streamlit as st\n",
        "import pickle\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import FAISS\n",
        "\n",
        "\n",
        "# Streamlit UI\n",
        "st.title(\"ArtiQuery ðŸ§\")\n",
        "st.sidebar.title(\"Article URLs\")\n",
        "\n",
        "# Collect URL inputs (ignoring empty values)\n",
        "urls = [url for url in [\n",
        "    st.sidebar.text_input(f\"URL {i+1}\", key=f\"url_{i}\") for i in range(3)\n",
        "] if url]\n",
        "\n",
        "process_url_clicked = st.sidebar.button(\"Process URLs\")\n",
        "\n",
        "# Define file path for saving the FAISS index\n",
        "file_path = \"faiss_store.pkl\"\n",
        "\n",
        "# Initialize the Gemini model using ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro-latest\", google_api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
        "\n",
        "# Use session state to manage FAISS readiness\n",
        "if \"faiss_ready\" not in st.session_state:\n",
        "    st.session_state.faiss_ready = False\n",
        "\n",
        "if process_url_clicked and urls:\n",
        "    st.info(\"Loading and processing articles... âŒ›\")\n",
        "    loaders = UnstructuredURLLoader(urls=urls)\n",
        "    data = loaders.load()\n",
        "\n",
        "    # Split documents into manageable chunks\n",
        "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "    docs = text_splitter.split_documents(data)\n",
        "\n",
        "    # Create embeddings and build FAISS index\n",
        "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "    faiss_index = FAISS.from_documents(docs, embeddings)\n",
        "\n",
        "    # Save FAISS index to file\n",
        "    with open(file_path, \"wb\") as f:\n",
        "        pickle.dump(faiss_index, f)\n",
        "\n",
        "    st.session_state.faiss_ready = True\n",
        "    st.success(\"Documents processed! You can now ask your question ðŸ‘‡\")\n",
        "\n",
        "if st.session_state.faiss_ready:\n",
        "    query = st.text_input(\"Ask a question about the articles:\")\n",
        "    if query:\n",
        "        # Load FAISS index from file\n",
        "        with open(file_path, \"rb\") as f:\n",
        "            vectorstore = pickle.load(f)\n",
        "\n",
        "        # Retrieve relevant documents\n",
        "        retriever = vectorstore.as_retriever()\n",
        "        retrieved_docs = retriever.get_relevant_documents(query)\n",
        "\n",
        "        # Combine document content for context\n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "        # Prompt with detailed instruction and clear separation\n",
        "        prompt = (\n",
        "            \"You are a knowledgeable and reliable expert assistant skilled in synthesizing complex information. \"\n",
        "            \"Below is the aggregated context extracted from various documents followed by a question. \"\n",
        "            \"Using the provided context, please generate a clear, comprehensive, and concise answer. \"\n",
        "            \"If relevant, include actionable insights and cite key points from the context. \\n\\n\"\n",
        "            \"Context:\\n\"\n",
        "            f\"{context}\\n\\n\"\n",
        "            \"Question:\\n\"\n",
        "            f\"{query}\\n\\n\"\n",
        "            \"Answer:\"\n",
        "        )\n",
        "\n",
        "        # Get the answer from the Gemini model\n",
        "        response = llm.invoke(prompt)\n",
        "\n",
        "        st.header(\"Answer\")\n",
        "        st.write(response.content)\n",
        "\n",
        "        # Deduplicate and display the source URLs\n",
        "        sources = [doc.metadata.get(\"source\", \"\") for doc in retrieved_docs if doc.metadata.get(\"source\", \"\")]\n",
        "        unique_sources = list(set(sources))\n",
        "        st.markdown(\"### Sources\")\n",
        "        for src in unique_sources:\n",
        "            st.markdown(f\"- [{src}]({src})\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFHmegKnbktK",
        "outputId": "dc23347e-2d7f-4a52-c024-1870ae1d1f9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: Run the app with ngrok\n",
        "from pyngrok import ngrok\n",
        "import time\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "\n",
        "# Set ngrok auth token\n",
        "ngrok.set_auth_token(os.getenv(\"NGROK_AUTH_TOKEN\"))\n",
        "\n",
        "# Kill existing tunnels\n",
        "ngrok.kill()\n",
        "\n",
        "# Start Streamlit server\n",
        "get_ipython().system_raw('streamlit run app.py --server.port 8501 &')\n",
        "\n",
        "# Wait for server to start\n",
        "time.sleep(5)\n",
        "\n",
        "# âœ… Create ngrok tunnel correctly\n",
        "public_url = ngrok.connect(addr=\"8501\", bind_tls=True)\n",
        "print(f\"Streamlit app running at: {public_url}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TdGMLBe79Msv",
        "outputId": "acb78d88-0099-4126-820b-cf0b75c6f204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit app running at: NgrokTunnel: \"https://91c8-35-237-30-54.ngrok-free.app\" -> \"http://localhost:8501\"\n"
          ]
        }
      ]
    }
  ]
}